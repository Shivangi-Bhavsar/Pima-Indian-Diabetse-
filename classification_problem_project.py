# -*- coding: utf-8 -*-
"""Classification Problem.Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L89OfG6hCUCESUIRMmP728pucl6yxf4a

**PIMA Indians Dataset - Classification Problem**
Using the PIMA Indian Diabetes dataset to create a model that predicts whether or not a person has diabetes using the medical attributes provided. § What are some insights that you noticed in the data? § What are the 3 most important factors for determining whether or not someone in the dataset is diabetic ?
"""

import numpy as np
import pandas as pd
import matplotlib as mp
from matplotlib import pyplot as plt
import statsmodels.api as sm
import seaborn as sns   
import csv
import os
import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn import tree
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.svm import SVC
from sklearn.neighbors import NearestNeighbors
from sklearn.neighbors import KNeighborsClassifier

#Loading the Pima Indian Diabetes dataset in 'df' dataframe for Analysis Purpose .
df = pd.read_csv(r'/diabetes.csv')
df.head()

print ("diabetes dataset has {} data points with {} variables each.".format(*df.shape))

df.describe()

## Counting cells with 0 Values for each variable and publishing the counts below
[(df.Pregnancies==0).sum(),
(df.Glucose==0).sum(),
(df.BloodPressure==0).sum(),
(df.SkinThickness==0).sum(),
(df.Insulin==0).sum(),
(df.BMI==0).sum(),
(df.DiabetesPedigreeFunction==0).sum(),
(df.Age==0).sum()]

''' Creating a dataset called 'df1' from original dataset 'df' with excludes all 
rows with have zeros only for Glucose, BP, Skinthickness, Insulin and BMI, as other columns can contain Zero values.'''

drop_Glu=df.index[df.Glucose == 0].tolist()
drop_BP=df.index[df.BloodPressure == 0].tolist()
drop_Skin = df.index[df.SkinThickness==0].tolist()
drop_Ins = df.index[df.Insulin==0].tolist()
drop_BMI = df.index[df.BMI==0].tolist()

c=drop_Glu+drop_BP+drop_Skin+drop_Ins+drop_BMI
##print(c)

df1=df.drop(df.index[c])

df1.info()

df1.describe()

## creating count plot with title using seaborn
  sns.countplot(x=df1.Outcome)
  plt.title("Count Plot for Outcome")

'''Split the data frame into two sub sets for convenience of analysis¶
#As we wish to study the influence of each variable on Outcome (Diabetic or not), we can subset the data by Outcome
df2 Subset : All samples with 1 values of Outcome
df3 Subset: All samples with 0 values of Outcome'''

df2 = df1[df1.Outcome==1]

df3 = df1[df1.Outcome==0]

df2.describe()

df3.describe()

sns.pairplot(df1, 
            vars=["Pregnancies",
                  "Glucose",
                  "BloodPressure",
                  "SkinThickness",
                  "Insulin",
                  "BMI",
                  "DiabetesPedigreeFunction", 
                  "Age"],
                  hue="Outcome")
plt.title("Pairplot of Variables by Outcome")

"""From scatter plots, to me only BMI & SkinThickness and Pregnancies & Age seem to have positive linear relationships. Another likely suspect is Glucose and Insulin.
There are no non-linear relationships
"""

correlation = df1.corr(method ='pearson')
correlation

sns.heatmap(correlation)

#checking for outliers whether to examin statical significance or not 
sns.boxplot(x=df1.Outcome,y=df1.BloodPressure)
plt.title("Boxplot of BP by Outcome")

sns.boxplot(x=df1.Outcome,y=df1.Glucose)
plt.title("Boxplot of Glucose by Outcome")

sns.boxplot(x=df1.Outcome,y=df1.Pregnancies)
plt.title("Boxplot of BP by Pregrnancies")

"""Logistic Regression


"""

#Applying logistic regression model to to predict the features that influence most to know whether person has diabetese or not 

cols=["Pregnancies", "Glucose","BloodPressure","SkinThickness","Insulin", "BMI","DiabetesPedigreeFunction", "Age"]
X=df1[cols]
Y=df1.Outcome

## Defining the model and assigning Y (Dependent) and X (Independent Variables)
logit_model=sm.Logit(Y,X)
result=logit_model.fit()
print(result.summary())

'''from the result summary , for features where the value of  
P < 0.05 consider to have higher influence on the dependent variable hence eliminating the rest of varibles and refitting the model '''

cols1=["Pregnancies", "Glucose","BloodPressure"]
X=df1[cols1]
Y=df1.Outcome

logit_model=sm.Logit(Y,X)
result=logit_model.fit()
print(result.summary())

logreg = LogisticRegression()
logreg.fit(X,Y)
Y_predicted=logreg.predict(X)
print("classification_report",'\n',classification_report(Y,Y_predicted))
print("confusion_matrix:","\n",confusion_matrix (Y,Y_predicted))

"""**Decision Tree**"""

#splitting dataset 

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=3)

#Next, train the model with the help of DecisionTreeClassifier class of sklearn as follows −

clf = tree.DecisionTreeClassifier()
clf = clf.fit(X_train,Y_train)
tree.plot_tree(clf)

Y_predicted1= clf.predict(X_test)

print(confusion_matrix(Y_test,Y_predicted1))
print(classification_report(Y_test,Y_predicted1))
print("Accuracy:",accuracy_score(Y_test,Y_predicted1))

"""**Support Vector Machine**"""

from sklearn import svm
clf2=svm.SVC()
clf2.fit(X_train,Y_train)
Y_predicted2= clf2.predict(X_test)

print(confusion_matrix(Y_test,Y_predicted2))
print(classification_report(Y_test,Y_predicted2))
print("Accuracy:",accuracy_score(Y_test,Y_predicted2))

"""**KNN**"""

#plotting error line to decide precise k value to avoid overfitting

error1= []
error2= []
for k in range(1,15):
    knn= KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train,Y_train)
    y_pred1= knn.predict(X_train)
    error1.append(np.mean(Y_train!= y_pred1))
    y_pred2= knn.predict(X_test)
    error2.append(np.mean(Y_test!= y_pred2))
#plt.figure(figsize(10,5))
plt.plot(range(1,15),error1,label="train")
plt.plot(range(1,15),error2,label="test")
plt.xlabel('k Value')
plt.ylabel('Error')
plt.legend()

# based on the errorline plot , choosing optimum value of K=9.

clf3 = KNeighborsClassifier(n_neighbors = 9 ).fit(X,Y) 
Y_predicted3= clf3.predict(X_test)
print(confusion_matrix(Y_test,Y_predicted3))
print(classification_report(Y_test,Y_predicted3))
print("Accuracy:",accuracy_score(Y_test,Y_predicted3))